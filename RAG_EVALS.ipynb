{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.create_dataset(\n",
    "    \"RAG-Evaluation\",\n",
    "    description=\"Un dataset para evaluar las respuestas del agente en temas de atencion al cliente utilizando RAG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input\": \"¿Cuáles son los canales de atención al cliente que ofrece TechnoVerde?\"},\n",
    "        {\"input\": \"¿Qué es el tiempo de respuesta para los diferentes canales de atención?\"},\n",
    "        {\"input\": \"¿Cómo puedo devolver un producto en TechnoVerde?\"},\n",
    "        {\"input\": \"¿Qué cubre la garantía de los productos TechnoVerde?\"},\n",
    "        {\"input\": \"¿Cómo funciona el programa de lealtad de TechnoVerde?\"},\n",
    "        {\"input\": \"¿Qué medidas toma TechnoVerde para proteger los datos personales?\"},\n",
    "        {\"input\": \"¿Qué formación reciben los empleados de atención al cliente?\"},\n",
    "        {\"input\": \"¿Qué hace TechnoVerde en caso de una crisis?\"},\n",
    "        {\"input\": \"¿Qué tecnologías innovadoras usa TechnoVerde en el servicio al cliente?\"},\n",
    "        {\"input\": \"¿Cómo garantiza TechnoVerde la accesibilidad en su servicio al cliente?\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"response\": \"TechnoVerde ofrece varios canales de atención al cliente: una línea telefónica gratuita (0800-TECNO-VERDE) disponible 24/7, correo electrónico (soporte@technoverde.com), chat en vivo en su sitio web (www.technoverde.com), redes sociales (@TechnoVerdeOficial), y atención presencial en sus tiendas de lunes a sábado de 9:00 a 20:00.\"},\n",
    "        {\"response\": \"El tiempo de respuesta objetivo es de menos de 3 minutos para llamadas telefónicas, menos de 4 horas hábiles para correos electrónicos, y menos de 1 minuto para el chat en vivo.\"},\n",
    "        {\"response\": \"Los clientes pueden devolver productos no utilizados dentro de los 30 días posteriores a la compra para un reembolso completo o un cambio, siempre que el producto esté en su empaque original y en condiciones de reventa.\"},\n",
    "        {\"response\": \"Todos los productos TechnoVerde tienen una garantía mínima de 2 años, con algunos productos premium que tienen hasta 5 años de garantía. Además, ofrecen soporte técnico gratuito durante la vida útil del producto.\"},\n",
    "        {\"response\": \"El programa de lealtad de TechnoVerde otorga puntos por cada compra (1 punto por cada $10 gastados). Existen niveles de membresía: Verde, Plata, Oro y Platino, cada uno con beneficios crecientes como descuentos, acceso a eventos exclusivos y envío gratuito.\"},\n",
    "        {\"response\": \"TechnoVerde recopila solo los datos necesarios para brindar sus servicios. Los datos se almacenan en servidores encriptados y se implementan medidas de seguridad avanzadas para proteger la información contra accesos no autorizados.\"},\n",
    "        {\"response\": \"Los empleados de atención al cliente reciben un programa intensivo de formación inicial de 4 semanas, que incluye conocimiento de productos, habilidades de comunicación y uso de sistemas. Además, hay programas de formación continua mensuales.\"},\n",
    "        {\"response\": \"TechnoVerde tiene un plan de continuidad del negocio que incluye la comunicación proactiva con los clientes y la búsqueda de soluciones alternativas en caso de crisis como desastres naturales o fallos tecnológicos.\"},\n",
    "        {\"response\": \"TechnoVerde utiliza inteligencia artificial para responder instantáneamente a preguntas frecuentes y analizar el sentimiento del cliente. También emplean realidad aumentada para guiar a los clientes en la instalación y solución de problemas.\"},\n",
    "        {\"response\": \"TechnoVerde asegura la accesibilidad mediante un diseño universal de su sitio web y aplicaciones móviles, compatibles con lectores de pantalla, y ofrece opciones de comunicación para clientes con discapacidades auditivas o del habla.\"}\n",
    "    ],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI App\n",
    "\n",
    "from agent import Agent\n",
    "from langchain_chroma import Chroma\n",
    "from tools import create_retriever_tool_from_vectorstore, create_get_client_info_tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "persist_directory = \"./chroma_db\"\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "tools = [create_retriever_tool_from_vectorstore(vectorstore), create_get_client_info_tool()]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "        (\"system\", \"Sos un asistente que responde preguntas sobre la empresa TechnoVerde S.A. Para preguntas relacionadas a la empresa, responde utilizando la informacion que tenes disponible sobre la misma, no inventes informacion. Si no conoces la respuesta, simplemente decí que no lo sabes y disculpate por no poder ayudar\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_agent(input_dict):\n",
    "    try:\n",
    "        agent = Agent(model_type=\"openai\", prompt=template, tools=tools)\n",
    "        human_message = HumanMessage(content=input_dict['input'])\n",
    "        state = agent.invoke([human_message])\n",
    "        \n",
    "        if isinstance(state, dict) and \"messages\" in state and len(state[\"messages\"]) > 0:\n",
    "            messages = state[\"messages\"]\n",
    "            result = {\"answer\": \"\", \"retrieved_content\": \"\"}\n",
    "            \n",
    "            for message in messages:\n",
    "                if isinstance(message, ToolMessage) and message.name == \"retrieve_company_docs\":\n",
    "                    result[\"retrieved_content\"] = message.content\n",
    "            \n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                result[\"answer\"] = last_message.content\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        return {\"answer\": \"No AI response found in the state.\", \"retrieved_content\": \"\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"retrieved_content\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_agent({\"input\": \"¿Cuáles son los canales de atención al cliente que ofrece TechnoVerde?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Evaluation result from LLM.\"\"\"\n",
    "\n",
    "    key: str = Field(description=\"The key for the evaluation\")\n",
    "    score: int = Field(description=\"The score for the evaluation\")\n",
    "    explanation: str = Field(description=\"Explanation of the score\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"key\": \"key_reference\",\n",
    "                \"score\": 5,\n",
    "                \"explanation\": \"The response was mostly accurate and relevant, but missed some minor details.\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "structured_llm = llm.with_structured_output(EvaluationResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluator function\n",
    "def faithfulness_evaluator(run: Run, example: Example):\n",
    "    # Extract prediction and reference outputs\n",
    "    prediction = run.outputs.get(\"answer\") or \"\"\n",
    "    reference = run.outputs.get(\"retrieved_content\") or \"\"\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the following: 'faithfulness'\n",
    "    Given the prediction:\n",
    "    {prediction}\n",
    "\n",
    "    And the reference:\n",
    "    {reference}\n",
    "\n",
    "    Evaluate the prediction by comparing it with the reference. \n",
    "    Provide a score between 1 and 10, where 1 is the lowest and 10 is the highest.\n",
    "    The score should reflect how well the prediction matches the reference in terms of content and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = structured_llm.invoke(prompt)\n",
    "    normalized_score = result.score / 10 \n",
    "    return {\"key\": \"faithfulness\", \"score\": normalized_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluator function\n",
    "def relevancy_evaluator(run: Run, example: Example):\n",
    "    # Extract prediction and query\n",
    "    prediction = run.outputs.get(\"answer\") or \"\"\n",
    "    query = example.inputs.get(\"input\") or \"\"\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the following: 'relevancy'\n",
    "    Given the prediction:\n",
    "    {prediction}\n",
    "\n",
    "    And the initial query:\n",
    "    {query}\n",
    "\n",
    "    Evaluate how relevant the prediction is to the initial query. \n",
    "    Provide a score between 1 and 10, where 1 is the lowest (not relevant at all) and 10 is the highest (extremely relevant).\n",
    "    The score should reflect how well the prediction addresses and answers the initial query.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = structured_llm.invoke(prompt)\n",
    "    normalized_score = result.score / 10 \n",
    "    return {\"key\": \"relevancy\", \"score\": normalized_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluator function for context recall\n",
    "def context_recall_evaluator(run: Run, example: Example):\n",
    "    # Extract retrieved content and ground truth\n",
    "    retrieved_content = run.outputs.get(\"retrieved_content\") or \"\"\n",
    "    ground_truth = example.outputs.get(\"response\") or \"\"\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the following: 'context recall'\n",
    "    Given the retrieved content:\n",
    "    {retrieved_content}\n",
    "\n",
    "    And the ground truth:\n",
    "    {ground_truth}\n",
    "\n",
    "    Evaluate how well the retriever was able to recall all the relevant context. \n",
    "    Provide a score between 1 and 10, where:\n",
    "    1 is the lowest (retrieved content contains no relevant information from the ground truth)\n",
    "    10 is the highest (retrieved content contains all relevant information from the ground truth)\n",
    "    \n",
    "    The score should reflect how completely the retrieved content captures the relevant information present in the ground truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = structured_llm.invoke(prompt)\n",
    "    normalized_score = result.score / 10 \n",
    "    return {\"key\": \"context_recall\", \"score\": normalized_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluator function for context precision\n",
    "def context_precision_evaluator(run: Run, example: Example):\n",
    "    # Extract retrieved content and query\n",
    "    retrieved_content = run.outputs.get(\"retrieved_content\") or \"\"\n",
    "    query = example.inputs.get(\"input\") or \"\"\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the following: 'context precision'\n",
    "    Given the retrieved content:\n",
    "    {retrieved_content}\n",
    "\n",
    "    And the initial query:\n",
    "    {query}\n",
    "\n",
    "    Evaluate how relevant the retrieved content is to the initial query. \n",
    "    Provide a score between 1 and 10, where:\n",
    "    1 is the lowest (retrieved content is not relevant at all to the query)\n",
    "    10 is the highest (retrieved content is highly relevant and precisely answers the query)\n",
    "    \n",
    "    The score should reflect how well the retrieved content addresses the specific information needs of the query, without including unnecessary or irrelevant information.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = structured_llm.invoke(prompt)\n",
    "    normalized_score = result.score / 10 \n",
    "    return {\"key\": \"context_precision\", \"score\": normalized_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RAG-Evaluation\"\n",
    "experiment_results = evaluate(\n",
    "    call_agent,\n",
    "    data=dataset_name,\n",
    "    evaluators=[context_recall_evaluator, context_precision_evaluator, relevancy_evaluator, faithfulness_evaluator],\n",
    "    experiment_prefix=\"ragas-full-evaluation\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "answer_correctness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"answer_correctness\": \"\"\"Is the Assistant's Answer aligned with the Ground Truth Answer? A score of [[1]] means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Groun Truth Answer. A score of [[5]] means \n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n",
    "            Answer. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth Answer.\"\"\"\n",
    "        },\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "        \"key\": \"answer_correctness\"\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"response\"],\n",
    "        \"input\": example.inputs[\"input\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG-Evaluation\"\n",
    "experiment_results = evaluate(\n",
    "   call_agent,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_correctness_evaluator],\n",
    "    experiment_prefix=\"answer-correctness\",\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easy-lang-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
